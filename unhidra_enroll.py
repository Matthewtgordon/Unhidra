import os
import time
import numpy as np
import soundfile as sf
from speechbrain.inference.speaker import SpeakerRecognition
import sounddevice as sd
import webrtcvad
import collections

# === Terminal formatting ===
RED = '\033[91m'
GREEN = '\033[92m'
CYAN = '\033[96m'
YELLOW = '\033[93m'
RESET = '\033[0m'

# === Robust sample rate detection ===
def find_working_samplerate():
    for rate in [48000, 44100, 32000, 16000]:
        try:
            with sd.InputStream(samplerate=rate, channels=1):
                return rate
        except Exception:
            continue
    return 16000

samplerate = find_working_samplerate()
print(f"{CYAN}ğŸš Using input sample rate: {samplerate}{RESET}")

# === Voice-activity-based recording ===
def record_until_silence(sample_rate=48000, frame_duration_ms=30, max_record_sec=10, silence_sec=0.8):
    vad = webrtcvad.Vad(2)
    frame_size = int(sample_rate * frame_duration_ms / 1000)
    num_padding_frames = int(silence_sec * 1000 / frame_duration_ms)
    ring_buffer = collections.deque(maxlen=num_padding_frames)
    recording = []
    triggered = False
    stream = sd.InputStream(samplerate=sample_rate, channels=1, dtype="int16", blocksize=frame_size)

    with stream:
        print(f"{CYAN}   ğŸ¤ Speak now (recording stops after {silence_sec}s of silence)...{RESET}")
        while True:
            audio_chunk, _ = stream.read(frame_size)
            raw = audio_chunk.flatten().tobytes()
            if len(raw) < 640:
                continue
            is_speech = vad.is_speech(raw, sample_rate)
            if not triggered:
                ring_buffer.append((raw, is_speech))
                if sum(1 for _, speech in ring_buffer if speech) > 0.9 * num_padding_frames:
                    triggered = True
                    for r, _ in ring_buffer:
                        recording.append(np.frombuffer(r, dtype="int16"))
                    ring_buffer.clear()
            else:
                recording.append(np.frombuffer(raw, dtype="int16"))
                ring_buffer.append((raw, is_speech))
                if sum(1 for _, speech in ring_buffer if not speech) > 0.9 * num_padding_frames:
                    break
            if len(recording) * frame_duration_ms > max_record_sec * 1000:
                print(f"{YELLOW}   â± Max recording duration reached.{RESET}")
                break
    return np.concatenate(recording)

# === Script ===
print(f"{CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
print(f"â•‘               UNHIDRA VOICE ENROLL              â•‘")
print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{RESET}\n")

print(f"{GREEN}ğŸ” Welcome, Initiate. Let's give Unhidra your voice.{RESET}\n")

name = input(f"{CYAN}ğŸ–‹  Enter a name for this voice profile (e.g. 'bronson'): {RESET}").strip().lower()
if not name:
    print(f"{RED}âŒ Invalid name. Aborting.{RESET}")
    exit(1)

print(f"\n{CYAN}ğŸ™ You'll speak 7 training phrases. Each will be recorded and analyzed.{RESET}\n")

phrases = [
    "Okay, let's get to it. Unhidra CLI â€” open it up. Time to get some work done.",
    "Execute primary command set. Authorization: granted.",
    "Voice link established. Standing by for task list.",
    "Activate terminal mode. Begin session logging now.",
    "I am the only one authorized to issue system orders.",
    "Begin operation. Time to see what Unhidra can really do.",
    "Full override. Engage voiceprint protocol â€” this is me."
]

model = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", savedir="~/.cache/speechbrain/speaker-id")

embeddings = []

for idx, phrase in enumerate(phrases):
    print(f"{CYAN}ğŸ“œ Line {idx+1}/7:\nâ€œ{phrase}â€{RESET}")
    input(f"   Press {YELLOW}Enter{RESET} to begin...")

    print(f"{YELLOW}   ğŸ¬ Recording in: 3... 2... 1...{RESET}")
    time.sleep(1)

    raw_audio = record_until_silence(sample_rate=samplerate)
    audio = raw_audio.astype("float32") / 32768.0
    filename = f"{name}_voice_{idx+1}.wav"
    sf.write(filename, audio, samplerate)
    print(f"{GREEN}   âœ… Saved: {filename}{RESET}\n")

    signal = model.load_audio(filename)
    signal = model.audio_normalizer(signal, samplerate)
    emb = model.encode_batch(signal).squeeze().detach().numpy()
    embeddings.append(emb)

avg_embedding = np.mean(embeddings, axis=0)
np.save(f"{name}_voiceprint.npy", avg_embedding)

print(f"\n{GREEN}ğŸ‰ Voice enrollment complete! Your profile '{name}_voiceprint.npy' is ready.{RESET}")
